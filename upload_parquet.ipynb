{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_20240601104418_part1.parquet:  18%|█▊        | 213M/1.21G [03:13<15:03, 1.10MB/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from huggingface_hub import HfApi, logging\n",
    "import glob\n",
    "\n",
    "jsonl_dir = \"out_data/\"\n",
    "jsonl_list = glob.glob(f\"{jsonl_dir}/*.jsonl\")\n",
    "jsonl_list.sort()\n",
    "\n",
    "logging.set_verbosity_debug()\n",
    "hf = HfApi()\n",
    "\n",
    "chunk_size = 1000000  # 100万件ごとに分割\n",
    "\n",
    "# 一時的にデータを保持するためのリスト\n",
    "temp_data = []\n",
    "\n",
    "for path in jsonl_list:\n",
    "    i=0\n",
    "    filename = path.split(\"/\")[-1]\n",
    "    dataset_name = filename.split(\".\")[0]\n",
    "\n",
    "    # JSONLファイルを読み込む\n",
    "    df = pd.read_json(path, lines=True)\n",
    "    \n",
    "    # 一時リストにデータを追加\n",
    "    temp_data.append(df)\n",
    "\n",
    "    # 一時リストのデータを結合\n",
    "    combined_df = pd.concat(temp_data, ignore_index=True)\n",
    "\n",
    "    # チャンクサイズを超える場合、Parquetに変換してアップロード\n",
    "    while len(combined_df) >= chunk_size:\n",
    "        chunk = combined_df[:chunk_size]\n",
    "        combined_df = combined_df[chunk_size:]\n",
    "        \n",
    "        table = pa.Table.from_pandas(chunk)\n",
    "        parquet_path = f\"{jsonl_dir}/{dataset_name}_part{i + 1}.parquet\"\n",
    "        pq.write_table(table, parquet_path)\n",
    "        \n",
    "        # Parquetファイルをアップロード\n",
    "        hf.upload_file(path_or_fileobj=parquet_path,\n",
    "                       path_in_repo=f\"data/{dataset_name}_part{i + 1}.parquet\",\n",
    "                       repo_id=\"kanhatakeyama/SyntheticText\",\n",
    "                       repo_type=\"dataset\")\n",
    "        i += 1\n",
    "\n",
    "# 残りのデータもParquetに変換してアップロード\n",
    "if len(combined_df) > 0:\n",
    "    table = pa.Table.from_pandas(combined_df)\n",
    "    parquet_path = f\"{jsonl_dir}/{dataset_name}_part{i + 1}.parquet\"\n",
    "    pq.write_table(table, parquet_path)\n",
    "    \n",
    "    hf.upload_file(path_or_fileobj=parquet_path,\n",
    "                   path_in_repo=f\"data/{dataset_name}_part{i + 1}.parquet\",\n",
    "                   repo_id=\"kanhatakeyama/SyntheticText\",\n",
    "                   repo_type=\"dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
